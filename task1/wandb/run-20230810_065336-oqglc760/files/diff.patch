diff --git a/task1/train.py b/task1/train.py
index d3beb29..68317c2 100755
--- a/task1/train.py
+++ b/task1/train.py
@@ -133,6 +133,35 @@ def main(config, do_eval, save_path):
     # -- start training
     batch_augment = Augment(config['augment'])
     for epoch in range(config['epochs']):
+
+        # -- valid step
+        if do_eval:
+            eval_size = len(valid_dataloader) * config['batch_size']
+            with torch.no_grad() :
+                net.eval()
+                daily_acc, gender_acc, embel_acc = 0.0, 0.0, 0.0
+                for i, batch in enumerate(tqdm(valid_dataloader, leave=False, desc='evaluating')):
+                    for key in batch: batch[key] = batch[key].to(device)
+                    daily_logit, gender_logit, embel_logit = net(batch['image'])
+                    daily_acc += accuracy(daily_logit, batch['daily'].to(device))
+                    gender_acc += accuracy(gender_logit, batch['gender'].to(device))
+                    embel_acc += accuracy(embel_logit, batch['embel'].to(device))
+                daily_acc /= eval_size
+                gender_acc /= eval_size
+                embel_acc /= eval_size
+                acc = (daily_acc + gender_acc + embel_acc) / 3 
+                if acc > epoch_eval_best_acc:
+                    epoch_eval_best_acc = acc
+                    torch.save(net.state_dict(), save_path + '/model_best_' + str(epoch + 1) + '.pkl')
+                eval_info = {
+                    "eval/acc": acc, 
+                    "eval/acc_daily" : daily_acc, 
+                    "eval/acc_gender": gender_acc,
+                    "eval/acc_embel" : embel_acc
+                }
+                wandb.log(eval_info)
+            net.train()
+
         # -- train step
         net.train()
         epoch_losses = [0, 0, 0, 0]
@@ -168,33 +197,7 @@ def main(config, do_eval, save_path):
               f"loss={epoch_losses[0]:.4f}, loss_daily={epoch_losses[1]:.4f}, ",
               f"loss_gender={epoch_losses[2]:.4f}, loss_embel={epoch_losses[3]:.4f}")
         
-        # -- valid step
-        if do_eval:
-            eval_size = len(valid_dataloader) * config['batch_size']
-            with torch.no_grad() :
-                net.eval()
-                daily_acc, gender_acc, embel_acc = 0.0, 0.0, 0.0
-                for i, batch in enumerate(tqdm(valid_dataloader, leave=False, desc='evaluating')):
-                    for key in batch: batch[key] = batch[key].to(device)
-                    daily_logit, gender_logit, embel_logit = net(batch['image'])
-                    daily_acc += acc(daily_logit, batch['daily'].to(device))
-                    gender_acc += acc(gender_logit, batch['gender'].to(device))
-                    embel_acc += acc(embel_logit, batch['embel'].to(device))
-                daily_acc /= eval_size
-                gender_acc /= eval_size
-                embel_acc /= eval_size
-                acc = (daily_acc + gender_acc + embel_acc) / 3 
-                if acc > epoch_eval_best_acc:
-                    epoch_eval_best_acc = acc
-                    torch.save(net.state_dict(), save_path + '/model_best_' + str(epoch + 1) + '.pkl')
-                eval_info = {
-                    "eval/acc": acc, 
-                    "eval/acc_daily" : daily_acc, 
-                    "eval/acc_gender": gender_acc,
-                    "eval/acc_embel" : embel_acc
-                }
-                wandb.log(eval_info)
-            net.train()
+        
 
         if ((epoch + 1) % 10 == 0):
             torch.save(net.state_dict(), save_path + '/model_' + str(epoch + 1) + '.pkl')
@@ -203,7 +206,7 @@ def main(config, do_eval, save_path):
     wandb.finish()
 
 
-def acc(self, logit, label) :
+def accuracy(self, logit, label) :
     """
     logit을 기준으로 label과 비교해서 accuracy를 구하기 위한 함수
     """
diff --git a/task1/wandb/debug-internal.log b/task1/wandb/debug-internal.log
index 917bf4e..f13b454 120000
--- a/task1/wandb/debug-internal.log
+++ b/task1/wandb/debug-internal.log
@@ -1 +1 @@
-run-20230810_042838-qpe6pnpl/logs/debug-internal.log
\ No newline at end of file
+run-20230810_065336-oqglc760/logs/debug-internal.log
\ No newline at end of file
diff --git a/task1/wandb/debug.log b/task1/wandb/debug.log
index 0b270e9..fd7c61d 120000
--- a/task1/wandb/debug.log
+++ b/task1/wandb/debug.log
@@ -1 +1 @@
-run-20230810_042838-qpe6pnpl/logs/debug.log
\ No newline at end of file
+run-20230810_065336-oqglc760/logs/debug.log
\ No newline at end of file
diff --git a/task1/wandb/latest-run b/task1/wandb/latest-run
index c2bd83e..c6420ac 120000
--- a/task1/wandb/latest-run
+++ b/task1/wandb/latest-run
@@ -1 +1 @@
-run-20230810_042838-qpe6pnpl
\ No newline at end of file
+run-20230810_065336-oqglc760
\ No newline at end of file
diff --git a/task2/dataset.py b/task2/dataset.py
old mode 100644
new mode 100755
